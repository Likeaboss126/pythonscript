import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import requests
import json

# -----------------------------
# 1Ô∏è‚É£  Load local ChromaDB collection
# -----------------------------
chroma_client = chromadb.Client(Settings(persist_directory="./chroma_storage"))
collection = chroma_client.get_collection(name="api_performance_data")

# -----------------------------
# 2Ô∏è‚É£  Load local embedding model
# -----------------------------
model_path = "models/mxbai-embed-large"
model = SentenceTransformer(model_path)

# -----------------------------
# 3Ô∏è‚É£  Define helper function to query
# -----------------------------
def query_vector_db(question: str, top_k: int = 5):
    # Create embedding for the user query
    query_embedding = model.encode([question], convert_to_numpy=True).tolist()

    # Search similar entries in vector DB
    results = collection.query(
        query_embeddings=query_embedding,
        n_results=top_k
    )

    # Flatten context results
    documents = results["documents"][0]
    context = "\n".join(documents)

    print(f"\nüîç Top {top_k} relevant records found for query:\n‚û°Ô∏è {question}\n")
    for i, doc in enumerate(documents, start=1):
        print(f"{i}. {doc}\n")

    return context


# -----------------------------
# 4Ô∏è‚É£  Optionally, summarize using local Ollama (LLaMA)
# -----------------------------
def ask_ollama(context: str, question: str):
    payload = {
        "model": "llama3",
        "prompt": f"Context:\n{context}\n\nQuestion:\n{question}\n\nAnswer concisely based on the data:",
        "stream": False
    }

    try:
        response = requests.post("http://localhost:11434/api/generate", json=payload)
        data = response.json()
        print("\nüß† AI Answer:\n")
        print(data.get("response", "No answer generated."))
    except Exception as e:
        print(f"‚ö†Ô∏è Error calling Ollama: {e}")


# -----------------------------
# 5Ô∏è‚É£  Run query interactively
# -----------------------------
if __name__ == "__main__":
    print("üí° Example questions you can ask:")
    print(" - Which API failed the most this week?")
    print(" - Which functionality has the slowest average response time?")
    print(" - List APIs with more than 5 failures.")
    print()

    question = input("Enter your question: ").strip()
    if not question:
        print("‚ùå No question entered. Exiting.")
        exit()

    # Get context from ChromaDB
    context_data = query_vector_db(question, top_k=5)

    # Optionally send to LLaMA for summary
    ask_ollama(context_data, question)


chroma_client = chromadb.Client(Settings(persist_directory="./chroma_storage"))
print("Available collections:", chroma_client.list_collections())


from langchain_chroma import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.docstore.document import Document
import os

persist_directory = r"C:\Users\Mudit\Documents\chroma_storage1"
os.makedirs(persist_directory, exist_ok=True)

embedding_model = OllamaEmbeddings(model="mxbai-embed-large")

docs = [Document(page_content="Hello world", metadata={"id": 1})]

db = Chroma.from_documents(
    documents=docs,
    embedding=embedding_model,
    persist_directory=persist_directory
)
db.persist()

print("‚úÖ Done. Check folder:", persist_directory)



from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

MODEL_PATH = r"C:\path\to\models\mxbai-embed-large"   # change to your local folder
PERSIST_DIR = r"C:\Users\Mudit\chroma_store"           # change to absolute path you want

# load model from local folder (no internet)
embedder = SentenceTransformer(MODEL_PATH)

client = chromadb.Client(Settings(persist_directory=PERSIST_DIR))
collection = client.get_or_create_collection("api_performance_data")

# Example docs
docs = ["API /login failed 5 times", "API /orders average 200ms"]
ids = ["d1", "d2"]

# Build embeddings locally then store
embeddings = embedder.encode(docs, convert_to_numpy=True).tolist()
collection.add(ids=ids, documents=docs, embeddings=embeddings)
client.persist()
print("Persisted to", PERSIST_DIR)
