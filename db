import ollama
import json
import streamlit as st

st.subheader("ðŸ’¬ AI Assistant: Ask About Performance Trends")

# Prepare context data (limit rows to avoid overloading)
context_data = df_filtered.head(100).to_dict(orient='records')

user_question = st.text_input("Ask a question about your API performance:")

if st.button("Ask AI") and user_question:
    with st.spinner("Analyzing data..."):
        # Prepare prompt for Ollama
        prompt = f"""
        You are an API performance analyst.
        Analyze this dataset (in JSON) and answer questions about trends, failures, and performance.
        Data:
        {json.dumps(context_data, indent=2)}

        Question: {user_question}
        Provide your answer clearly in plain English, highlighting insights.
        """

        try:
            response = ollama.chat(model="llama3", messages=[{"role": "user", "content": prompt}])
            st.markdown(f"**AI Insight:** {response['message']['content']}")
        except Exception as e:
            st.error(f"Error connecting to Ollama: {e}")

df_filtered["DateTime"] = df_filtered["DateTime"].astype(str)
import json

# If context_data is a dict that contains a DataFrame:
if isinstance(context_data, dict):
    for key, value in context_data.items():
        if isinstance(value, pd.DataFrame):
            context_data[key] = value.to_dict(orient='records')

# Or if it's directly a DataFrame:
if isinstance(context_data, pd.DataFrame):
    context_data = context_data.to_dict(orient='records')

# Now this will work:
json_str = json.dumps(context_data, indent=2)
