pip install chromadb sentence-transformers psycopg2-binary pandas

print(f"‚úÖ Loaded {len(df)} rows from database.")

# ---------- 3Ô∏è‚É£ Initialize Chroma and Embedding Model ----------
client = chromadb.PersistentClient(path="./chroma_store")
collection = client.get_or_create_collection("api_perf")

embedder = SentenceTransformer("mixedbread-ai/mxbai-embed-large")

# ---------- 4Ô∏è‚É£ Prepare Documents ----------
docs = []
ids = []

for idx, row in df.iterrows():
    doc = (
        f"Functionality: {row['Functionality']}\n"
        f"DateTime: {row['DateTime']}\n"
        f"API: {row['API']}\n"
        f"Total Requests: {row['Total']}\n"
        f"Passed: {row['Passed']}\n"
        f"Failed: {row['Failed']}\n"
        f"Average Time Taken: {row['AverageTimeTaken']}"
    )
    docs.append(doc)
    ids.append(str(idx))

# ---------- 5Ô∏è‚É£ Embed and Store ----------
print("üß† Generating embeddings... (this can take time for large data)")

batch_size = 128
for i in tqdm(range(0, len(docs), batch_size)):
    batch_docs = docs[i:i + batch_size]
    batch_ids = ids[i:i + batch_size]
    embeddings = embedder.encode(batch_docs, batch_size=32)
    collection.add(ids=batch_ids, documents=batch_docs, embeddings=embeddings)

print(f"‚úÖ Vector DB initialized with {len(docs)} records.")




model_path = "models/mxbai-embed-large"
model = SentenceTransformer(model_path)

# -----------------------------
# 3Ô∏è‚É£  Initialize local ChromaDB
# -----------------------------
chroma_client = chromadb.Client(Settings(persist_directory="./chroma_storage"))
collection = chroma_client.get_or_create_collection(name="api_performance_data")

# -----------------------------
# 4Ô∏è‚É£  Prepare text data for embedding
# -----------------------------
texts = []
ids = []

for idx, row in tqdm(df.iterrows(), total=len(df)):
    text = (
        f"Functionality: {row['Functionality']}, "
        f"API: {row['API']}, "
        f"AverageTimeTaken: {row['AverageTimeTaken']} ms, "
        f"Total: {row['Total']}, OK: {row['OK']}, KO: {row['KO']}, "
        f"DateTime: {row['DateTime']}"
    )
    texts.append(text)
    ids.append(str(idx))

# -----------------------------
# 5Ô∏è‚É£  Generate embeddings and store in Chroma
# -----------------------------
BATCH_SIZE = 50

for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="Embedding batches"):
    batch_texts = texts[i:i + BATCH_SIZE]
    batch_ids = ids[i:i + BATCH_SIZE]

    embeddings = model.encode(batch_texts, convert_to_numpy=True).tolist()

    collection.add(
        ids=batch_ids,
        documents=batch_texts,
        embeddings=embeddings
    )

print(f"‚úÖ Successfully initialized vector database with {len(texts)} records.")
