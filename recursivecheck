import requests
import time
import urllib3
from urllib.parse import quote_plus
from bs4 import BeautifulSoup

# Modern LangChain Imports
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIGURATION ---
BASE_URL = "https://<domain>.atlassian.net/wiki"
TOKEN = "YOUR_BEARER_TOKEN"
SPACE_KEY = "ABCD"
PARENT_TITLE = "Data Issues"
DB_PATH = "./confluence_vector_db"

headers = {
    "Accept": "application/json",
    "Authorization": f"Bearer {TOKEN}"
}

def safe_get(url):
    r = requests.get(url, headers=headers, verify=False)
    r.raise_for_status()
    return r.json()

def clean_html(raw_html):
    if not raw_html: return ""
    return BeautifulSoup(raw_html, "html.parser").get_text(separator=' ', strip=True)

# 1Ô∏è‚É£ GET PARENT ID
def get_parent_id():
    url = f"{BASE_URL}/rest/api/content?spaceKey={SPACE_KEY}&title={quote_plus(PARENT_TITLE)}&limit=1"
    data = safe_get(url)
    if data["size"] == 0: raise Exception(f"‚ùå Parent page '{PARENT_TITLE}' not found.")
    return data["results"][0]["id"]

# 2Ô∏è‚É£ GET CHILDREN USING SEARCH/CQL (Compliant with restrictions)
def find_children_via_cql(parent_id):
    """
    Uses the /rest/api/content endpoint with a CQL filter.
    'parent=ID' specifically finds direct children.
    """
    print(f"üéØ Finding children using CQL for Parent ID: {parent_id}...")
    
    # We use the 'parent' field in CQL to get direct children only
    cql_query = f"parent={parent_id} AND type=page"
    url = f"{BASE_URL}/rest/api/content?cql={quote_plus(cql_query)}&expand=body.storage&limit=50"
    
    data = safe_get(url)
    docs = []
    
    for page in data.get("results", []):
        title = page["title"]
        page_id = page["id"]
        raw_body = page.get("body", {}).get("storage", {}).get("value", "")
        
        print(f"‚úÖ Captured: {title}")
        
        docs.append(Document(
            page_content=clean_html(raw_body),
            metadata={
                "title": title,
                "id": page_id,
                "source": f"{BASE_URL}/pages/viewpage.action?pageId={page_id}"
            }
        ))
    return docs

# --- PIPELINE ---
if __name__ == "__main__":
    try:
        p_id = get_parent_id()
        raw_documents = find_children_via_cql(p_id)
        
        if not raw_documents:
            print("‚ö†Ô∏è No children found. Double-check if the pages are direct children.")
        else:
            print(f"üôå Processing {len(raw_documents)} pages...")
            
            # Split and Store
            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            chunks = splitter.split_documents(raw_documents)

            embeddings = OllamaEmbeddings(model="mxbai-embed-large")
            vector_db = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings,
                persist_directory=DB_PATH
            )
            print(f"üéâ SUCCESS! Database ready at: {DB_PATH}")

    except Exception as e:
        print(f"‚ùå Error: {e}")
