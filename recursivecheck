import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIG ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "/path/to/your/local/model" # Must be the same path used for creation

# --- 1. LOAD LOCAL EMBEDDINGS ---
print(f"üöÄ Loading local model from: {MODEL_PATH}")
embeddings = HuggingFaceEmbeddings(
    model_name=MODEL_PATH,
    model_kwargs={'device': 'cpu'}
)

# --- 2. CONNECT TO LOCAL DB ---
print(f"üìÇ Connecting to ChromaDB at: {DB_PATH}")
vector_db = Chroma(
    persist_directory=DB_PATH, 
    embedding_function=embeddings
)

def verify_retrieval(query, k=3):
    """
    k = number of matching chunks to return
    """
    print(f"\nüîç Searching for: '{query}'")
    print("=" * 60)
    
    # This performs the mathematical similarity search
    results = vector_db.similarity_search(query, k=k)
    
    if not results:
        print("‚ùå No matching chunks found. The database might be empty.")
        return

    for i, doc in enumerate(results, 1):
        print(f"MATCH #{i}")
        print(f"üìÑ PAGE TITLE: {doc.metadata.get('title')}")
        print(f"üîó SOURCE URL: {doc.metadata.get('source')}")
        # Print a snippet of the text found
        content_snippet = doc.page_content[:500].replace('\n', ' ')
        print(f"üìù TEXT FOUND: {content_snippet}...")
        print("-" * 60)

# --- MAIN LOOP ---
if __name__ == "__main__":
    print("\n‚úÖ Verification System Ready.")
    print("This script checks the 'Brain' of your search without using an AI LLM.")
    
    while True:
        user_query = input("\nEnter a search term to test (or 'exit'): ")
        if user_query.lower() in ['exit', 'quit']:
            break
        
        # We ask for the top 3 most relevant chunks
        verify_retrieval(user_query, k=3)






# Syntax: docker cp "C:\path\to\your\model.gguf" <container_name_or_id>:/root/model.gguf
docker cp "C:\Users\YourName\Downloads\llama-3.2-1b.gguf" ollama:/root/local-model.gguf

eds a "recipe" to understand how to handle that file. You need to create a small text file named Modelfile inside the cont
docker exec -it ollama sh -c "echo 'FROM /root/local-model.gguf' > /root/Modelfile"

docker exec -it ollama ollama create my-custom-model -f /root/Modelfile

docker exec -it ollama ollama list






import streamlit as st
import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIGURATION ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "/path/to/your/local/embedding/model" # Your mxbai-embed-large folder
LLM_NAME = "my-custom-model" # üéØ The name you gave it in 'ollama create'

# --- PAGE SETUP ---
st.set_page_config(page_title="Confluence AI Assistant", page_icon="ü§ñ")
st.title("ü§ñ Confluence Data Assistant")
st.markdown("Querying 7 specific 'Data Issues' pages via Local RAG.")

# --- INITIALIZE MODELS (Cached to prevent reloading on every click) ---
@st.cache_resource
def load_rag_system():
    # 1. Embeddings
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    
    # 2. Vector DB
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    
    # 3. LLM (Pointing to your Docker Ollama)
    # Default is localhost:11434; if Docker is on another IP, use base_url="http://IP:11434"
    llm = OllamaLLM(model=LLM_NAME, temperature=0)
    
    # 4. Prompt
    template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
    Use the context to answer the question concisely. If unsure, say you don't know.<|eot_id|>
    <|start_header_id|>user<|end_header_id|>
    CONTEXT: {context}
    QUESTION: {question}<|eot_id|>
    <|start_header_id|>assistant<|end_header_id|>"""
    
    prompt = PromptTemplate(input_variables=["context", "question"], template=template)
    
    # 5. Chain
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

qa_chain = load_rag_system()

# --- CHAT INTERFACE ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User Input
if prompt := st.chat_input("Ask about participations..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = qa_chain.invoke({"query": prompt})
            answer = response["result"]
            
            # Extract unique sources
            sources = list(set([doc.metadata.get('title') for doc in response["source_documents"]]))
            
            full_response = f"{answer}\n\n**Sources:** {', '.join(sources)}"
            st.markdown(full_response)
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})



import streamlit as st
import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIGURATION ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "C:/path/to/your/local/mxbai-model" # Use absolute Windows path
LLM_NAME = "my-custom-model" # The name you used in 'ollama create'
# If Docker is on your local machine, 'localhost' is correct.
OLLAMA_URL = "http://localhost:11434" 

# --- PAGE SETUP ---
st.set_page_config(page_title="Confluence AI", page_icon="üè¢")
st.title("üè¢ Confluence POC Assistant")

# --- LOAD SYSTEM ---
@st.cache_resource
def init_rag():
    # 1. Local Embeddings (The 'Search Engine')
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    
    # 2. Vector DB
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    
    # 3. Docker-hosted LLM (The 'Librarian')
    llm = OllamaLLM(model=LLM_NAME, base_url=OLLAMA_URL, temperature=0)
    
    # 4. The Prompt (2026 optimized)
    template = """Answer the question based ONLY on the following context:
    {context}
    
    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    # 5. LCEL Chain (The Modern Way)
    # This replaces the old RetrievalQA
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain, retriever

rag_chain, retriever = init_rag()

# --- CHAT UI ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if user_input := st.chat_input("Ask about participations..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    
    with st.chat_message("assistant"):
        with st.spinner("Analyzing 7 pages..."):
            # Get answer
            full_answer = rag_chain.invoke(user_input)
            
            # Get sources separately for citation
            docs = retriever.invoke(user_input)
            source_list = list(set([d.metadata.get('title') for d in docs]))
            
            final_text = f"{full_answer}\n\n**Sources:** {', '.join(source_list)}"
            st.write(final_text)
            st.session_state.messages.append({"role": "assistant", "content": final_text})




import streamlit as st
import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIGURATION ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "C:/path/to/your/local/mxbai-model" 
LLM_NAME = "my-custom-model" 
OLLAMA_URL = "http://localhost:11434" 

# --- LOAD SYSTEM ---
@st.cache_resource
def init_rag():
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    
    llm = OllamaLLM(model=LLM_NAME, base_url=OLLAMA_URL, temperature=0)
    
    template = """Answer the question based ONLY on the following context:
    {context}
    
    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain, retriever

rag_chain, retriever = init_rag()

# --- CHAT UI ---
st.title("üè¢ Confluence Data Assistant")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if user_input := st.chat_input("Ask about data issues..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    
    with st.chat_message("assistant"):
        with st.spinner("Analyzing Confluence pages..."):
            # 1. Generate the Answer
            full_answer = rag_chain.invoke(user_input)
            
            # 2. Retrieve the original Documents to get Metadata
            docs = retriever.invoke(user_input)
            
            # 3. Create a unique list of (Title, URL) pairs
            sources = []
            for d in docs:
                title = d.metadata.get('title', 'Unknown Page')
                url = d.metadata.get('source', '#')
                if (title, url) not in sources:
                    sources.append((title, url))
            
            # 4. Display the Answer
            st.write(full_answer)
            
            # 5. Display the Clickable Sources
            st.markdown("---")
            st.write("**References:**")
            for title, url in sources:
                st.markdown(f"- [{title}]({url})")
            
            # Save to history
            source_text = "\n\n**Sources:** " + ", ".join([t for t, u in sources])
            st.session_state.messages.append({"role": "assistant", "content": full_answer + source_text})







import requests
import time
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://<domain>.atlassian.net/wiki"
TOKEN = "YOUR_BEARER_TOKEN"
SPACE_KEY = "ABCD"

headers = {
    "Accept": "application/json",
    "Authorization": f"Bearer {TOKEN}"
}

def safe_get(url, max_retries=4):
    wait_time = 5

    for attempt in range(max_retries):
        r = requests.get(url, headers=headers, verify=False)

        if r.status_code == 429:
            retry_after = int(r.headers.get("Retry-After", wait_time))
            print(f"‚è≥ Rate limited. Sleeping {retry_after}s")
            time.sleep(retry_after)
            wait_time *= 2
            continue

        r.raise_for_status()
        return r.json()

    raise Exception("‚ùå Max retries exceeded")

def print_page_titles_with_ancestors():
    start = 0
    limit = 25
    page_count = 0

    while True:
        url = (
            f"{BASE_URL}/rest/api/content"
            f"?spaceKey={SPACE_KEY}&limit={limit}&start={start}&expand=ancestors"
        )

        data = safe_get(url)

        for page in data["results"]:
            page_count += 1
            print(f"{page_count}. {page['title']}")

        if "next" not in data["_links"]:
            break

        start += limit
        time.sleep(2)  # prevent rate burst

    print(f"\n‚úÖ Total pages fetched: {page_count}")

if __name__ == "__main__":
    print("üîé Fetching page titles (with ancestors, cheap)...\n")
    print_page_titles_with_ancestors()




import logging
import requests
import time
import urllib3
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# --- LOGGING CONFIG ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("indexing.log"), # Writes to file
        logging.StreamHandler()              # Also prints to terminal
    ]
)
logger = logging.getLogger(__name__)

# --- CONFIG ---
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
BASE_URL = "https://<domain>.atlassian.net/wiki"
TOKEN = "YOUR_BEARER_TOKEN"
SPACE_KEY = "ABCD"
DB_PATH = "./confluence_vector_db"
EMBED_MODEL_PATH = "C:/path/to/your/local/mxbai-model"

headers = {"Accept": "application/json", "Authorization": f"Bearer {TOKEN}"}

def build_index():
    start = 0
    limit = 25
    langchain_docs = []
    
    logger.info(f"üöÄ Starting crawl for space: {SPACE_KEY}")
    
    try:
        while True:
            url = f"{BASE_URL}/rest/api/content?spaceKey={SPACE_KEY}&limit={limit}&start={start}&expand=body.storage"
            r = requests.get(url, headers=headers, verify=False)
            
            if r.status_code == 429:
                retry = int(r.headers.get("Retry-After", 10))
                logger.warning(f"‚ö†Ô∏è Rate limited. Sleeping {retry}s")
                time.sleep(retry)
                continue
            
            data = r.json()
            if not data["results"]: break
                
            for page in data["results"]:
                title = page['title']
                content = page.get("body", {}).get("storage", {}).get("value", "")
                langchain_docs.append(Document(page_content=content, metadata={"title": title}))
                logger.info(f"‚úÖ Cached: {title}")

            if "next" not in data["_links"]: break
            start += limit
            time.sleep(1)

        logger.info(f"üì¶ Total pages found: {len(langchain_docs)}. Starting split...")
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split_documents(langchain_docs)
        
        logger.info(f"üíæ Generating Embeddings for {len(chunks)} chunks. This will take time...")
        embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_PATH)
        Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_PATH)
        
        logger.info("üèÅ SUCCESS: Vector database build complete!")

    except Exception as e:
        logger.error(f"‚ùå FATAL ERROR: {str(e)}", exc_info=True)

if __name__ == "__main__":
    build_index()






import streamlit as st
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# --- GLOBAL CONFIG ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "C:/path/to/your/local/mxbai-model"

# This is the "Magic" that stops the loading lag
@st.cache_resource
def load_system():
    # 1. Load the "brain" (Embedding Model)
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    
    # 2. Open the 160MB "Library" (ChromaDB)
    # This only happens ONCE
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    
    return vector_db

# --- INITIALIZATION ---
# This line will run instantly after the first load
vector_db = load_system()

# --- MAIN UI ---
st.title("üè¢ Confluence Assistant")
st.write("Ready to answer questions.")



# Instead of a hardcoded string, let's use the exact Cloud format
# Ensure your BASE_URL ends with '/wiki'
for title, url in sources:
    # Most Confluence Cloud links work best in this format:
    # https://<your-domain>.atlassian.net/wiki/spaces/<spacekey>/pages/<pageid>
    # OR the universal static link:
    clean_url = f"{BASE_URL}/pages/viewpage.action?pageId={url.split('=')[-1]}"
    
    st.markdown(f"- [{title}]({clean_url})")





vimport requests
from markdownify import markdownify as md
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# --- CONFIG ---
BASE_URL = "https://<domain>.atlassian.net/wiki"
DB_PATH = "./confluence_gold_v1"
MODEL_PATH = "C:/path/to/your/local/mxbai-model"

def build_expert_db():
    # Use the search API to get the newest stuff first
    cql = f"space = '{SPACE_KEY}' AND type = page ORDER BY lastmodified DESC"
    url = f"{BASE_URL}/rest/api/content/search?cql={cql}&expand=body.storage,history.lastUpdated,history.createdBy,ancestors"
    
    # ... (Standard Request Logic) ...
    
    processed_docs = []
    for page in results:
        # 1. Convert messy XML to clean Markdown for Llama
        html_body = page.get("body", {}).get("storage", {}).get("value", "")
        markdown_body = md(html_body, heading_style="ATX")
        
        # 2. Build Breadcrumb Path (e.g., HR > Policies > 2026)
        path = " > ".join([a['title'] for a in page.get("ancestors", [])] + [page['title']])
        
        # 3. Contextual Injection (The "Secret Sauce")
        # We put key info at the TOP of every chunk so it's always in the AI's window
        header = f"--- DOCUMENT START ---\nPATH: {path}\nOWNER: {page['history']['createdBy']['displayName']}\nMODIFIED: {page['history']['lastUpdated']['when']}\n\n"
        
        doc = Document(
            page_content=header + markdown_body,
            metadata={
                "source": f"{BASE_URL}/pages/viewpage.action?pageId={page['id']}",
                "path": path,
                "author": page['history']['createdBy']['displayName'],
                "date": page['history']['lastUpdated']['when']
            }
        )
        processed_docs.append(doc)

    # 4. Smart Splitting (Markdown aware)
    # This prevents splitting a Markdown table in half
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=150,
        separators=["\n### ", "\n## ", "\n# ", "\n\n", "\n", " "]
    )
    chunks = splitter.split_documents(processed_docs)
    
    # 5. Build the Index
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    Chroma.from_documents(chunks, embeddings, persist_directory=DB_PATH)
    print(f"‚úÖ Foolproof DB built with {len(chunks)} contextual chunks.")
