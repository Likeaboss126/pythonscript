import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIG ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "/path/to/your/local/model" # Must be the same path used for creation

# --- 1. LOAD LOCAL EMBEDDINGS ---
print(f"üöÄ Loading local model from: {MODEL_PATH}")
embeddings = HuggingFaceEmbeddings(
    model_name=MODEL_PATH,
    model_kwargs={'device': 'cpu'}
)

# --- 2. CONNECT TO LOCAL DB ---
print(f"üìÇ Connecting to ChromaDB at: {DB_PATH}")
vector_db = Chroma(
    persist_directory=DB_PATH, 
    embedding_function=embeddings
)

def verify_retrieval(query, k=3):
    """
    k = number of matching chunks to return
    """
    print(f"\nüîç Searching for: '{query}'")
    print("=" * 60)
    
    # This performs the mathematical similarity search
    results = vector_db.similarity_search(query, k=k)
    
    if not results:
        print("‚ùå No matching chunks found. The database might be empty.")
        return

    for i, doc in enumerate(results, 1):
        print(f"MATCH #{i}")
        print(f"üìÑ PAGE TITLE: {doc.metadata.get('title')}")
        print(f"üîó SOURCE URL: {doc.metadata.get('source')}")
        # Print a snippet of the text found
        content_snippet = doc.page_content[:500].replace('\n', ' ')
        print(f"üìù TEXT FOUND: {content_snippet}...")
        print("-" * 60)

# --- MAIN LOOP ---
if __name__ == "__main__":
    print("\n‚úÖ Verification System Ready.")
    print("This script checks the 'Brain' of your search without using an AI LLM.")
    
    while True:
        user_query = input("\nEnter a search term to test (or 'exit'): ")
        if user_query.lower() in ['exit', 'quit']:
            break
        
        # We ask for the top 3 most relevant chunks
        verify_retrieval(user_query, k=3)






# Syntax: docker cp "C:\path\to\your\model.gguf" <container_name_or_id>:/root/model.gguf
docker cp "C:\Users\YourName\Downloads\llama-3.2-1b.gguf" ollama:/root/local-model.gguf

eds a "recipe" to understand how to handle that file. You need to create a small text file named Modelfile inside the cont
docker exec -it ollama sh -c "echo 'FROM /root/local-model.gguf' > /root/Modelfile"

docker exec -it ollama ollama create my-custom-model -f /root/Modelfile

docker exec -it ollama ollama list






import streamlit as st
import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIGURATION ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "/path/to/your/local/embedding/model" # Your mxbai-embed-large folder
LLM_NAME = "my-custom-model" # üéØ The name you gave it in 'ollama create'

# --- PAGE SETUP ---
st.set_page_config(page_title="Confluence AI Assistant", page_icon="ü§ñ")
st.title("ü§ñ Confluence Data Assistant")
st.markdown("Querying 7 specific 'Data Issues' pages via Local RAG.")

# --- INITIALIZE MODELS (Cached to prevent reloading on every click) ---
@st.cache_resource
def load_rag_system():
    # 1. Embeddings
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    
    # 2. Vector DB
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    
    # 3. LLM (Pointing to your Docker Ollama)
    # Default is localhost:11434; if Docker is on another IP, use base_url="http://IP:11434"
    llm = OllamaLLM(model=LLM_NAME, temperature=0)
    
    # 4. Prompt
    template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
    Use the context to answer the question concisely. If unsure, say you don't know.<|eot_id|>
    <|start_header_id|>user<|end_header_id|>
    CONTEXT: {context}
    QUESTION: {question}<|eot_id|>
    <|start_header_id|>assistant<|end_header_id|>"""
    
    prompt = PromptTemplate(input_variables=["context", "question"], template=template)
    
    # 5. Chain
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

qa_chain = load_rag_system()

# --- CHAT INTERFACE ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User Input
if prompt := st.chat_input("Ask about participations..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = qa_chain.invoke({"query": prompt})
            answer = response["result"]
            
            # Extract unique sources
            sources = list(set([doc.metadata.get('title') for doc in response["source_documents"]]))
            
            full_response = f"{answer}\n\n**Sources:** {', '.join(sources)}"
            st.markdown(full_response)
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})



import streamlit as st
import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIGURATION ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "C:/path/to/your/local/mxbai-model" # Use absolute Windows path
LLM_NAME = "my-custom-model" # The name you used in 'ollama create'
# If Docker is on your local machine, 'localhost' is correct.
OLLAMA_URL = "http://localhost:11434" 

# --- PAGE SETUP ---
st.set_page_config(page_title="Confluence AI", page_icon="üè¢")
st.title("üè¢ Confluence POC Assistant")

# --- LOAD SYSTEM ---
@st.cache_resource
def init_rag():
    # 1. Local Embeddings (The 'Search Engine')
    embeddings = HuggingFaceEmbeddings(model_name=MODEL_PATH)
    
    # 2. Vector DB
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    
    # 3. Docker-hosted LLM (The 'Librarian')
    llm = OllamaLLM(model=LLM_NAME, base_url=OLLAMA_URL, temperature=0)
    
    # 4. The Prompt (2026 optimized)
    template = """Answer the question based ONLY on the following context:
    {context}
    
    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    # 5. LCEL Chain (The Modern Way)
    # This replaces the old RetrievalQA
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain, retriever

rag_chain, retriever = init_rag()

# --- CHAT UI ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if user_input := st.chat_input("Ask about participations..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    
    with st.chat_message("assistant"):
        with st.spinner("Analyzing 7 pages..."):
            # Get answer
            full_answer = rag_chain.invoke(user_input)
            
            # Get sources separately for citation
            docs = retriever.invoke(user_input)
            source_list = list(set([d.metadata.get('title') for d in docs]))
            
            final_text = f"{full_answer}\n\n**Sources:** {', '.join(source_list)}"
            st.write(final_text)
            st.session_state.messages.append({"role": "assistant", "content": final_text})
