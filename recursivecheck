import urllib3
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIG ---
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "/path/to/your/local/model" # Must be the same path used for creation

# --- 1. LOAD LOCAL EMBEDDINGS ---
print(f"üöÄ Loading local model from: {MODEL_PATH}")
embeddings = HuggingFaceEmbeddings(
    model_name=MODEL_PATH,
    model_kwargs={'device': 'cpu'}
)

# --- 2. CONNECT TO LOCAL DB ---
print(f"üìÇ Connecting to ChromaDB at: {DB_PATH}")
vector_db = Chroma(
    persist_directory=DB_PATH, 
    embedding_function=embeddings
)

def verify_retrieval(query, k=3):
    """
    k = number of matching chunks to return
    """
    print(f"\nüîç Searching for: '{query}'")
    print("=" * 60)
    
    # This performs the mathematical similarity search
    results = vector_db.similarity_search(query, k=k)
    
    if not results:
        print("‚ùå No matching chunks found. The database might be empty.")
        return

    for i, doc in enumerate(results, 1):
        print(f"MATCH #{i}")
        print(f"üìÑ PAGE TITLE: {doc.metadata.get('title')}")
        print(f"üîó SOURCE URL: {doc.metadata.get('source')}")
        # Print a snippet of the text found
        content_snippet = doc.page_content[:500].replace('\n', ' ')
        print(f"üìù TEXT FOUND: {content_snippet}...")
        print("-" * 60)

# --- MAIN LOOP ---
if __name__ == "__main__":
    print("\n‚úÖ Verification System Ready.")
    print("This script checks the 'Brain' of your search without using an AI LLM.")
    
    while True:
        user_query = input("\nEnter a search term to test (or 'exit'): ")
        if user_query.lower() in ['exit', 'quit']:
            break
        
        # We ask for the top 3 most relevant chunks
        verify_retrieval(user_query, k=3)
