import requests
import time
import urllib3
from urllib.parse import quote_plus
from bs4 import BeautifulSoup

# LangChain / Local Model Imports
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- CONFIG ---
BASE_URL = "https://<domain>.atlassian.net/wiki"
TOKEN = "YOUR_BEARER_TOKEN"
SPACE_KEY = "ABCD"
PARENT_TITLE = "Data Issues"
DB_PATH = "./confluence_vector_db"
MODEL_PATH = "/path/to/your/local/model" # Path to your mxbai folder
CHILD_LIMIT = 7  # üõë POC LIMIT

headers = {
    "Accept": "application/json",
    "Authorization": f"Bearer {TOKEN}"
}

def clean_html(raw_html):
    if not raw_html: return ""
    return BeautifulSoup(raw_html, "html.parser").get_text(separator=' ', strip=True)

# 1Ô∏è‚É£ GET PARENT ID
def get_parent_id():
    url = f"{BASE_URL}/rest/api/content?spaceKey={SPACE_KEY}&title={quote_plus(PARENT_TITLE)}&limit=1"
    r = requests.get(url, headers=headers, verify=False)
    data = r.json()
    if data["size"] == 0: raise Exception("‚ùå Parent page not found")
    return data["results"][0]["id"]

# 2Ô∏è‚É£ SCAN SPACE WITH ANCESTOR LOGIC + LIMIT
def scan_for_children(parent_id):
    documents = []
    start = 0
    limit = 25 # API batch size
    
    print(f"üîé Scanning space for {CHILD_LIMIT} children of {PARENT_TITLE}...")
    
    while len(documents) < CHILD_LIMIT:
        url = f"{BASE_URL}/rest/api/content?spaceKey={SPACE_KEY}&limit={limit}&start={start}&expand=ancestors,body.storage"
        r = requests.get(url, headers=headers, verify=False)
        data = r.json()

        for page in data["results"]:
            # --- YOUR ANCESTOR LOGIC ---
            if page.get("ancestors") and page["ancestors"][-1]["id"] == parent_id:
                title = page["title"]
                raw_body = page.get("body", {}).get("storage", {}).get("value", "")
                
                documents.append(Document(
                    page_content=clean_html(raw_body),
                    metadata={
                        "title": title,
                        "source": f"{BASE_URL}/pages/viewpage.action?pageId={page['id']}"
                    }
                ))
                print(f"‚úÖ Found ({len(documents)}/{CHILD_LIMIT}): {title}")
                
                # Stop immediately if we hit the POC limit
                if len(documents) >= CHILD_LIMIT:
                    break

        if "next" not in data["_links"] or len(documents) >= CHILD_LIMIT:
            break

        start += limit
        time.sleep(0.5)
        
    return documents

# 3Ô∏è‚É£ MAIN EXECUTION
if __name__ == "__main__":
    p_id = get_parent_id()
    raw_docs = scan_for_children(p_id)
    
    if raw_docs:
        # Split into chunks
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split_documents(raw_docs)

        print(f"üöÄ Loading Local Model: {MODEL_PATH}")
        # Equivalent to SentenceTransformers(MODEL_PATH)
        embeddings = HuggingFaceEmbeddings(
            model_name=MODEL_PATH,
            model_kwargs={'device': 'cpu'}
        )

        print("üß† Creating Vector DB...")
        vector_db = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=DB_PATH
        )
        print(f"üéâ POC Complete. {len(raw_docs)} pages stored in {DB_PATH}")
