import chromadb
from ollama import Client, chat, ChatResponse
import random
import time
from datetime import datetime, timedelta
import json
from typing import Optional, Dict, Any, List
from typing import Literal # Used for cleaner type hinting

COLLECTION_NAME = "api_performance_v5"
DB_PATH = "./chroma_db_performance_v5"
OLLAMA_HOST = 'http://localhost:11434'
OLLAMA_MODEL = 'llama3.1'  # Use your specific Llama 3 model tag
NUM_RECORDS = 5000
API_LIST = ["POST /login", "GET /userProfile", "POST /submitOrder", "GET /productDetails"]
FUNCTIONALITY_LIST = ["Authentication", "UserManagement", "OrderProcessing", "Catalog"]
RelativePeriod = Literal['TODAY', 'YESTERDAY', 'LAST_7_DAYS', 'LAST_30_DAYS', 'LAST_MONTH']

def generate_synthetic_data(count):
    records = []

    for i in range(1, count + 1):
        api = random.choice(API_LIST)
        func = FUNCTIONALITY_LIST[API_LIST.index(api)]

        # Numeric Fields (Simulating different data across last 60 days)
        total = random.randint(100, 2000)
        failed = random.choices([0, 1, 5, 20, 100], weights=[60, 15, 10, 8, 7])[0]
        avg_time = random.uniform(2000.0, 5000.0) if failed >= 20 else random.uniform(50.0, 800.0)

        start_date = datetime.now() - timedelta(days=60)
        execution_time = start_date + timedelta(seconds=random.randint(0, 60 * 24 * 60 * 60))

        metadata = {
            "functionality": func,
            "api": api,
            "total": total,
            "passed": total - failed,
            "failed": failed,
            "averagetimetaken": round(avg_time, 2),
            "datetime": execution_time.isoformat(),
            "datetime_epoch": int(execution_time.timestamp()),  # Numeric for filtering
        }

        records.append({
            "id": f"run_{i}",
            "document": f"Performance metrics for {func} suite, {api} endpoint. Status: {'Critical' if failed > 20 else 'Normal'}.",
            "metadata": metadata
        })
    file_path = "my_list_data.json"

    with open(file_path, 'w', encoding='utf-8') as json_file:
        json.dump(records, json_file, indent=4, ensure_ascii=False)

    print(f"List saved to {file_path}")
    return records


def index_data(records: List[Dict]):
    print(f"Initializing ChromaDB at {DB_PATH}...")
    client = chromadb.PersistentClient(path=DB_PATH)
    try:
        client.delete_collection(name=COLLECTION_NAME)
    except Exception:
        pass

    collection = client.get_or_create_collection(name=COLLECTION_NAME)

    collection.add(
        documents=[r["document"] for r in records],
        metadatas=[r["metadata"] for r in records],
        ids=[r["id"] for r in records],
    )
    print(f"Indexing complete. Total records: {collection.count()}")


if __name__ == "__main__":
    synthetic_data = generate_synthetic_data(NUM_RECORDS)
    index_data(synthetic_data)




from tqdm import tqdm
from typing import List, Dict, Any
import chromadb
from sentence_transformers import SentenceTransformer 
# Import the library needed to load the model

# --- Configuration ---
COLLECTION_NAME = "api_performance_v5"
DB_PATH = "./chroma_db_performance_v5"
BATCH_SIZE = 500

# 1. Load the specific model you downloaded
# If the model is in a local directory, replace the name with the path:
# E.g., model_path = '/path/to/local/mxbai-embed-large'
EMBEDDING_MODEL_NAME = 'mixedbread-ai/mxbai-embed-large-v1' # Or local path

try:
    # Load the model using SentenceTransformer. This will use your local copy.
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    # mxbai-embed-large outputs 1024 dimensions (Source 2.1)
    print(f"Successfully loaded embedding model: {EMBEDDING_MODEL_NAME}. Output dimension: 1024")
except Exception as e:
    print(f"Error loading {EMBEDDING_MODEL_NAME}: {e}")
    # Fallback to the default if the local load fails
    embedding_model = None 


def index_data_batched(records: List[Dict]):
    if not embedding_model:
        print("Embedding model failed to load. Cannot proceed with indexing.")
        return

    print(f"Initializing ChromaDB at {DB_PATH}...")
    client = chromadb.PersistentClient(path=DB_PATH)
    
    # --- Prepare Data ---
    texts = [r["document"] for r in records]
    metadatas = [r["metadata"] for r in records]
    ids = [r["id"] for r in records]

    # --- Create Collection without an explicit embedding_function ---
    # We will provide the embeddings directly, so Chroma skips the auto-generation step.
    try:
        client.delete_collection(name=COLLECTION_NAME)
    except Exception:
        pass
        
    collection = client.get_or_create_collection(name=COLLECTION_NAME) 
    
    # --- Batching and Encoding Loop ---
    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="Embedding batches"):
        batch_texts = texts[i:i + BATCH_SIZE]
        batch_metadatas = metadatas[i:i + BATCH_SIZE]
        batch_ids = ids[i:i + BATCH_SIZE]

        # 2. Use the local model to encode the documents
        # This is where your dedicated model is utilized
        embeddings = embedding_model.encode(
            batch_texts, 
            convert_to_numpy=True
        ).tolist() # Convert numpy array back to list of floats for ChromaDB

        # 3. Add to ChromaDB
        try:
            collection.add(
                ids=batch_ids,
                documents=batch_texts,
                metadatas=batch_metadatas,
                embeddings=embeddings # Provide the pre-calculated embeddings
            )
        except Exception as e:
             print(f"\nChromaDB add error on batch {i // BATCH_SIZE + 1}: {e}")
             break
        
    print(f"\nIndexing complete. Total records: {collection.count()}")

# Example call: index_data_batched(synthetic_data)
